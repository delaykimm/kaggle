{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG7Ni0gzNJNh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
        "from tensorflow.keras.losses import MeanSquaredError,BinaryCrossentropy,MeanAbsoluteError\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './train.csv'\n",
        "file_path2 = './test.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# train_data (+ val_data)\n",
        "df = pd.read_csv(file_path)\n",
        "df.index = df[\"id\"]\n",
        "df = pd.DataFrame(df.iloc[:, 1:])\n",
        "\n",
        "# test_data\n",
        "df2 = pd.read_csv(file_path2)\n",
        "df2.index = df2[\"id\"]\n",
        "df2 = pd.DataFrame(df2.iloc[:,1:])\n",
        "X_test = df2\n",
        "\n",
        "# Split the data into training and validating sets\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "oCKi4OguNapN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "4NNcpErejK07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### eliminating outliers"
      ],
      "metadata": {
        "id": "nUCBI2MgjAj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df[['allelectrons_Total', 'density_Total', 'allelectrons_Average',\n",
        "       'val_e_Average', 'atomicweight_Average', 'ionenergy_Average',\n",
        "       'el_neg_chi_Average', 'R_vdw_element_Average', 'R_cov_element_Average',\n",
        "       'zaratio_Average', 'density_Average']].quantile(0.2)\n",
        "Q3 = df[['allelectrons_Total', 'density_Total', 'allelectrons_Average',\n",
        "       'val_e_Average', 'atomicweight_Average', 'ionenergy_Average',\n",
        "       'el_neg_chi_Average', 'R_vdw_element_Average', 'R_cov_element_Average',\n",
        "       'zaratio_Average', 'density_Average']].quantile(0.8)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# 'Column1'과 'Column3'에 대해서만 IQR 기반 이상치 제거\n",
        "outlier_condition = ~((df[['allelectrons_Total', 'density_Total', 'allelectrons_Average',\n",
        "       'val_e_Average', 'atomicweight_Average', 'ionenergy_Average',\n",
        "       'el_neg_chi_Average', 'R_vdw_element_Average', 'R_cov_element_Average',\n",
        "       'zaratio_Average', 'density_Average']] < (Q1 - 1.5 * IQR)) | (df[['allelectrons_Total', 'density_Total', 'allelectrons_Average',\n",
        "       'val_e_Average', 'atomicweight_Average', 'ionenergy_Average',\n",
        "       'el_neg_chi_Average', 'R_vdw_element_Average', 'R_cov_element_Average',\n",
        "       'zaratio_Average', 'density_Average']] > (Q3 + 1.5 * IQR)))\n",
        "df_no_outliers = df[outlier_condition.all(axis=1)]\n",
        "\n",
        "# 결과 출력\n",
        "#print(\"Original DataFrame:\")\n",
        "#print(df)\n",
        "#print(\"\\nDataFrame without Outliers:\")\n",
        "#print(df_no_outliers)\n",
        "print(len(df_no_outliers))"
      ],
      "metadata": {
        "id": "aWTvgCPti_3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_no_outliers['Hardness'].plot(kind='hist', bins=20, title='val_e_Average')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "1AQsukBSbe-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building the model"
      ],
      "metadata": {
        "id": "YFcP9DLUkYZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, BatchNormalization, Dense\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class DropConnectLayer(Layer):\n",
        "    def __init__(self, dropconnect_rate, **kwargs):\n",
        "        super(DropConnectLayer, self).__init__(**kwargs)\n",
        "        self.dropconnect_rate = dropconnect_rate\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\"kernel\", shape=(input_shape[-1],), initializer=\"glorot_uniform\", trainable=True)\n",
        "        super(DropConnectLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            mask = tf.keras.backend.random_binomial(shape=tf.shape(self.kernel), p=1 - self.dropconnect_rate)\n",
        "            return inputs * self.kernel * mask\n",
        "        else:\n",
        "            return inputs * self.kernel\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "def build_model_with_dropconnect(input_shape, dropconnect_rate):\n",
        "    model = Sequential([\n",
        "        BatchNormalization(epsilon=1e-5, input_shape=(11,)),\n",
        "        Dense(16, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "        #DropConnectLayer(dropconnect_rate),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='gelu'),\n",
        "        #DropConnectLayer(dropconnect_rate),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "        #DropConnectLayer(dropconnect_rate),\n",
        "        Dropout(0.5),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = (11,)\n",
        "dropconnect_rate = 0  # You can adjust this rate based on your needs\n",
        "model = build_model_with_dropconnect(input_shape, dropconnect_rate)\n",
        "\n",
        "#def build_model_using_sequential(input_shape):\n",
        "#    model = Sequential([\n",
        "#        BatchNormalization(epsilon=1e-5, input_shape=input_shape),\n",
        "#        Dense(16, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "#        Dense(32, activation='gelu'),\n",
        "#        Dense(64, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "#        Dense(1)\n",
        "#    ])\n",
        "#    return model\n",
        "\n",
        "# Example of how to use the function with an input shape\n",
        "input_shape = (11, )\n",
        "model = build_model_with_dropconnect(input_shape, dropconnect_rate)\n",
        "\n",
        "# Optionally, you can build the model to show the summary\n",
        "#model.build((None, 11))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "fcEcSLlfO_YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    return tfp.stats.percentile(tf.abs(y_true - y_pred), q=50)\n",
        "\n",
        "#def metric_fn(y_true, y_pred):\n",
        "#    return tfp.stats.percentile(tf.abs(y_true - y_pred), q=100) - tfp.stats.percentile(tf.abs(y_true - y_pred), q=0)\n",
        "\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        patience=50,\n",
        "        restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, min_lr=0.00001),\n",
        "]"
      ],
      "metadata": {
        "id": "huaB0H2zPESj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = 5\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(df_no_outliers)):\n",
        "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    X_train_fold, X_val_fold = df_no_outliers.iloc[train_indices,:-1], df_no_outliers.iloc[val_indices, :-1]\n",
        "    y_train_fold, y_val_fold = df_no_outliers.iloc[train_indices, -1], df_no_outliers.iloc[val_indices, -1]\n",
        "\n",
        "    # Create an instance of your TensorFlow model\n",
        "    #model = build_model_using_sequential(input_shape)  # Replace with how you create your model function\n",
        "    model = build_model_with_dropconnect(input_shape, dropconnect_rate)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.013, beta_1=0.5), loss=loss_fn, metrics=['mae'])\n",
        "\n",
        "    # Create and fit a TensorFlow Dataset for training\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_fold.values, y_train_fold.values)).shuffle(len(X_train_fold)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val_fold.values, y_val_fold.values)).batch(batch_size)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_mae = model.evaluate(val_dataset)\n",
        "    print(f\"Validation Loss: {val_loss}, Validation MAE: {val_mae}\")\n",
        "\n",
        "# After all folds, if you want to train on the full dataset for more epochs:\n",
        "history = model.fit(\n",
        "    df_no_outliers.iloc[:, :-1].values.astype('float32'),\n",
        "    df_no_outliers.iloc[:, -1].values.astype('float32'),\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks_list,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "QbpBcRrFPKp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test.astype('float32'))\n",
        "df_prediction = pd.DataFrame(y_pred)\n",
        "df_prediction"
      ],
      "metadata": {
        "id": "E_ho5BqLPQzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_prediction[0].plot(kind='hist', bins=20, title=0)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Bv34G6Bpm0I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_prediction[0].plot(kind='hist', bins=20, title=0)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rhxKKTwooHH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test= pd.read_csv('./sample_submission.csv')\n",
        "test['Hardness'] = df_prediction\n",
        "test.to_csv('submission.csv',index=False)"
      ],
      "metadata": {
        "id": "NNqFBsFtTpHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Training and validation losses')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['mae'], label='Training accuracy')\n",
        "plt.plot(history.history['val_mae'], label='Validation accuracy')\n",
        "plt.title('Training and validation accuracies')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Icdn6rH8n6BP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}